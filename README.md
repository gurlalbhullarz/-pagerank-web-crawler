# PageRank Web Crawler & Visualization
![PageRank Visualization](assets/graph-preview.png)

![MIT License](https://img.shields.io/badge/license-MIT-green)

This project is a **Python-based simulation of a search engine ranking system** inspired by Google‚Äôs original **PageRank algorithm**.

It crawls web pages, stores link relationships in a database, computes PageRank scores using an iterative algorithm, and visualizes the web structure as an interactive graph.

This project helped me connect Python programming with real-world concepts in web development and algorithms.
I hope this project can also serve as a learning guide for others who want to explore crawling, graph structures, and data visualization.

This is an **educational project** focused on understanding how search engines work internally.


---

## üìå What This Project Does

- Crawls web pages starting from a seed URL  
- Extracts and stores links between pages  
- Builds a directed graph of the website  
- Computes PageRank scores iteratively  
- Visualizes the web graph using D3.js  

---

## üõ†Ô∏è Technologies Used

- Python  
- SQLite  
- BeautifulSoup  
- urllib  
- D3.js  
- HTML / JavaScript  

---

## üìÇ Project Structure

```
pagerank-web-crawler/
‚îú‚îÄ‚îÄ spider.py        # Web crawler
‚îú‚îÄ‚îÄ sprank.py        # PageRank algorithm
‚îú‚îÄ‚îÄ spdump.py        # Link inspection tool
‚îú‚îÄ‚îÄ spjson.py        # Graph data generator
‚îú‚îÄ‚îÄ force.html       # Visualization (D3.js)
‚îú‚îÄ‚îÄ spider.js        # Generated graph data
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

---

## ‚úÖ Prerequisites

- Python 3.7 or later
- Install dependency:

```bash
pip install beautifulsoup4


‚∏ª

‚ñ∂Ô∏è How to Run

1. Crawl the website

python spider.py

	‚Ä¢	Enter a starting URL when prompted
	‚Ä¢	Crawling is limited to one domain for ethical reasons
    ‚Ä¢	The crawler fetches pages from the same domain.
	‚Ä¢	It stores the link structure for PageRank computation.
	‚Ä¢	Tip for beginners: Start with small websites so the visualization is manageable.

‚∏ª
2. Compute PageRank

Run the PageRank calculation:

python sprank.py

	‚Ä¢	Enter the number of iterations (e.g., 10).
	‚Ä¢	The script calculates PageRank scores for each page.
	‚Ä¢	Learning point: Try different iteration counts and see how the PageRank scores stabilize.

‚∏ª

3. Inspect the Graph (Optional)

To see the raw link structure:

python spdump.py

	‚Ä¢	Prints pages and their outgoing links.
	‚Ä¢	Helpful to understand how the crawler builds the graph.

‚∏ª

4. Export Graph for Visualization

python spjson.py

	‚Ä¢	Generates graph.json with nodes and links.
	‚Ä¢	Nodes include PageRank scores for visualization.

‚∏ª

5. Visualize the Graph

Serve the files using a local server:

python -m http.server

	‚Ä¢	Open your browser at http://localhost:8000/force.html.
	‚Ä¢	Nodes are sized by PageRank; links show page connections.
	‚Ä¢	Drag nodes for better visualization.
	‚Ä¢	Tip: For larger graphs, consider showing only the top-ranked pages to keep the visualization smooth.

‚∏ª

üìò What I Learned
	‚Ä¢	How web crawlers work internally
	‚Ä¢	How PageRank distributes importance across links
	‚Ä¢	How iterative algorithms converge
	‚Ä¢	Practical use of graph theory
	‚Ä¢	Connecting backend data with frontend visualization

I also realized the importance of small, step-by-step experiments in programming ‚Äî testing my crawler with a few pages first, checking outputs with spdump.py, and then moving to visualization.

‚∏ª

Beginner Tips
	‚Ä¢	Start with small websites to avoid slow visualization.
	‚Ä¢	Inspect your graph with spdump.py before visualizing to understand what is happening behind the scenes.
	‚Ä¢	Experiment with different damping factors in sprank.py to see how PageRank changes.
	‚Ä¢	Make small modifications (crawl depth, max pages) to explore Python coding and crawling logic.
	‚Ä¢	Don‚Äôt be afraid to break things ‚Äî experimenting is the best way to learn.

‚∏ª

Possible Improvements
	‚Ä¢	Handle invalid URLs and network errors gracefully.
	‚Ä¢	Limit crawl depth or maximum number of pages for safety.
	‚Ä¢	Respect robots.txt and add polite delays between requests.
	‚Ä¢	Add labels or tooltips in the visualization to show URLs or PageRank.
	‚Ä¢	Modularize the code into reusable functions.
	‚Ä¢	Optimize PageRank computation using NumPy or sparse matrices for larger graphs.

‚∏ª
‚∏ª

‚ö†Ô∏è Notes
	‚Ä¢	Crawling is domain-restricted by design
	‚Ä¢	Visualization performance depends on node count
	‚Ä¢	This is not a production search engine

‚∏ª

üôè Acknowledgment

Inspired by the Python for Everybody course
by Dr. Charles R. Severance (Coursera)

‚∏ª

üë§ Author

Gurlal Singh
Computer Science Student
Python | Data Structures | Algorithms

‚≠ê If you find this project useful, feel free to star the repository.
